{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e345726-9a77-4ab8-aba5-1679d53b45ef",
   "metadata": {},
   "source": [
    "# 102 category flower classification using ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d17cc-c8e5-44ed-9bae-19f91357c487",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf65ce5-5cc4-4ac0-b594-0448edbc6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09903e9-0f13-4a45-ad04-486c21891ad8",
   "metadata": {},
   "source": [
    "## 2. Define Data Paths\n",
    "We set paths to the data directory and metadata files (label and split files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604d698d-ac02-4a01-889a-22fac769a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to data directory\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Set path to label files & split files\n",
    "label_file = data_dir / \"imagelabels.mat\"\n",
    "split_file = data_dir / \"setid.mat\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814f26e-6762-47fa-9e7f-f56e451730ec",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Metadata\n",
    "We'll load the `.mat` files containig the labels and train/test splits using `scipy.io.loadmat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb9d5e1-da8c-4e10-9fda-db6427dc0353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Feb 19 15:43:33 2009',\n",
       "  '__version__': '1.0',\n",
       "  '__globals__': [],\n",
       "  'labels': array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)},\n",
       " {'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Feb 19 17:38:58 2009',\n",
       "  '__version__': '1.0',\n",
       "  '__globals__': [],\n",
       "  'trnid': array([[6765, 6755, 6768, ..., 8026, 8036, 8041]], dtype=uint16),\n",
       "  'valid': array([[6773, 6767, 6739, ..., 8028, 8008, 8030]], dtype=uint16),\n",
       "  'tstid': array([[6734, 6735, 6737, ..., 8044, 8045, 8047]], dtype=uint16)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data = scipy.io.loadmat(label_file)\n",
    "splits_data = scipy.io.loadmat(split_file)\n",
    "\n",
    "labels_data, splits_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a1d6c-1103-4ee3-9d5d-c15cfbbfbcc4",
   "metadata": {},
   "source": [
    "## 4. Extract label and split indices\n",
    "We'll extract the 102 labels of the flower and the indices of the images which define which images belong to which split (train/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974a8fd8-4424-4441-a7ff-738b2aece535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five labels: [77 77 77 77 77]\n",
      "Min value of train_indices: 28\n",
      "Min value of test_indices: 1\n",
      "Min value of val_indices: 17\n",
      "\n",
      "Length of train_indices: 1020\n",
      "Length of test_indices: 6149\n",
      "Length of val_indices: 1020\n"
     ]
    }
   ],
   "source": [
    "labels = labels_data['labels'].flatten()\n",
    "train_indices = splits_data['trnid'].flatten()\n",
    "test_indices = splits_data['tstid'].flatten()\n",
    "val_indices = splits_data['valid'].flatten()\n",
    "\n",
    "print(f'First five labels: {labels[:5]}')\n",
    "print(f'Min value of train_indices: {train_indices.min()}')\n",
    "print(f'Min value of test_indices: {test_indices.min()}')\n",
    "print(f'Min value of val_indices: {val_indices.min()}')\n",
    "print()\n",
    "print(f'Length of train_indices: {len(train_indices)}')\n",
    "print(f'Length of test_indices: {len(test_indices)}')\n",
    "print(f'Length of val_indices: {len(val_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e88124-ea02-42dc-9fa7-fd27b2a22063",
   "metadata": {},
   "source": [
    "## 5. Map indices to Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4b966f-f1d5-4ebb-967a-059a143b9511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/images/image_00001.jpg'),\n",
       " WindowsPath('data/images/image_00002.jpg'),\n",
       " WindowsPath('data/images/image_00003.jpg'),\n",
       " WindowsPath('data/images/image_00004.jpg'),\n",
       " WindowsPath('data/images/image_00005.jpg')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and sort all the image files\n",
    "data_path = data_dir / \"images\"\n",
    "images_files = sorted(list(data_path.glob(\"*.jpg\")))\n",
    "\n",
    "images_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e15c6a-1ed2-4cf6-9f02-2fb489ca8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min train_images value: data\\images\\image_00028.jpg\n",
      "Min test_values: data\\images\\image_00001.jpg\n",
      "Min val_values: data\\images\\image_00017.jpg\n",
      "\n",
      "Length of train_images: 1020\n",
      "Length of test_images 6149\n",
      "Length of val_images: 1020\n"
     ]
    }
   ],
   "source": [
    "# Create lists of image paths and corresponding labels for the training set.\n",
    "# i-1 adjustment is done for matching the indices from matlab to python's zero based system\n",
    "train_images = [images_files[i-1] for i in train_indices]  \n",
    "train_labels = [labels[i-1] for i in train_indices]\n",
    "\n",
    "test_images = [images_files[i-1] for i in test_indices]\n",
    "test_labels = [labels[i-1] for i in test_indices]\n",
    "\n",
    "\n",
    "val_images = [images_files[i-1] for i in val_indices]\n",
    "val_labels = [labels[i-1] for i in val_indices]\n",
    "\n",
    "# Checking the data\n",
    "print(f'Min train_images value: {min(train_images)}')\n",
    "print(f'Min test_values: {min(test_images)}')\n",
    "print(f'Min val_values: {min(val_images)}')\n",
    "print()\n",
    "print(f'Length of train_images: {len(train_images)}')\n",
    "print(f'Length of test_images {len(test_images)}')\n",
    "print(f'Length of val_images: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077dc56-7ea7-45b7-b3bd-af49bf974858",
   "metadata": {},
   "source": [
    "## 6. Define Transformations\n",
    "We use the default transformations for ResNet50, including resizing, cropping, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec9ffa6-c1dd-44b5-ad85-fab2d4c31ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[232]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "pretrained_weights = models.ResNet50_Weights.DEFAULT\n",
    "common_transforms = pretrained_weights.transforms()\n",
    "\n",
    "common_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0ca32-2dab-441d-95cd-c662c2fc1675",
   "metadata": {},
   "source": [
    "## 7. Create custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d79ff9b-2b62-4074-8235-cd5a689aa2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset: 1020\n",
      "Length of test_dataset 6149\n",
      "Length of val_dataset: 1020\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, labels, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        label = self.labels[idx] - 1\n",
    "        img = transforms.functional.pil_to_tensor(transforms.functional.load_image(data_path))\n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=common_transforms)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=common_transforms)\n",
    "test_dataset = CustomDataset(test_images, test_labels, transform=common_transforms)\n",
    "\n",
    "print(f'Length of train_dataset: {len(train_dataset)}')\n",
    "print(f'Length of test_dataset {len(test_dataset)}')\n",
    "print(f'Length of val_dataset: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72ad6b-a96a-4ba5-9400-ead669e6a413",
   "metadata": {},
   "source": [
    "## 8. Create Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17419e07-1df8-4217-9d65-7b45a29893ab",
   "metadata": {},
   "source": [
    "We create the `train_loader`, `val_loader`, `test_loader` with mini-batch size of 32.\n",
    "* `train_loader` feeds batches of training data to the model during training.\n",
    "* `val_loader` is used during training to monitor performance on unseen data and detect overfitting.\n",
    "* `test_loader` is used after training to evaluate final model performance on a separate test set.\n",
    " \n",
    "The reason of using `shuffle=True` for `train_loader` and `shuffle=False` (default) for `val_loader` and `test_loader`:\n",
    "* Shuffling is used to break the sequential order of data, helping to improve model generalization and avoid overfitting to data patterns.\n",
    "* Validation and testing datasets do not require shuffling since they are only used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15821064-3f4f-45e0-bb45-769e2843871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define min-batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training dataset\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, # Load batch_size samples at a time\n",
    "                          shuffle=True)    # Randomly shuffle the data at the start of each epoch to improve model generalization\n",
    "\n",
    "# Create the DataLoader for the validatoin dataset\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)  # No shuffling needed for validation, as order doesn't affect model evaluation\n",
    "\n",
    "# Create the DataLoader for the testing dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc198a40-2e99-4031-b45b-d91dbf01a5ac",
   "metadata": {},
   "source": [
    "## 9. Load Pretrained Model\n",
    "We follow the following steps to load the pretrained model:\n",
    "* Load the ResNet50 model with `pretrained_weights`\n",
    "* Replace the final fully connected layer to output 102 classes\n",
    "* Freeze the pretrained Layers except the final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83008d96-8ca0-45b1-8f7a-27906123d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "# Load the ResNet50 model with pretrained weights \n",
    "model = models.resnet50(weights=pretrained_weights)\n",
    "\n",
    "# Replace the final fully connected layer to output 102 classes\n",
    "num_classes = 102\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a605450-0a57-4b71-bdb6-630b6c16b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the final one to leverage pretrained features\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f41505-2449-461c-b51f-f8c594aed8a8",
   "metadata": {},
   "source": [
    "## 10. Move Model to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c23f82-2e49-460d-b7ed-eb84241e69a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the device to cuda if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "347eff56-75b7-4aa4-9c98-9f20eb570982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to current device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d170d-b785-48ae-bb39-f5808c7b481b",
   "metadata": {},
   "source": [
    "## 11. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4af4bd7-1f5f-40a5-ba8d-7068cd7f76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "accuracy_fn = Accuracy(task='multiclass', num_classes=102)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c2636-1ddd-4050-bde9-2ed328109b03",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12865cf1-46b9-4857-99c4-fbe461c8aafa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2338673895.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from torch import tqdm\n",
    "from train import train_model, test_model\n",
    "\n",
    "epochs = 10\n",
    "train_loss, train_accuracy = 0, 0\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70eb634-ed08-4c30-a8e5-58cd5db3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.BC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
