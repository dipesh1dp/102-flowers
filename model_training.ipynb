{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e345726-9a77-4ab8-aba5-1679d53b45ef",
   "metadata": {},
   "source": [
    "# 102 category flower classification using ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d17cc-c8e5-44ed-9bae-19f91357c487",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bf65ce5-5cc4-4ac0-b594-0448edbc6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09903e9-0f13-4a45-ad04-486c21891ad8",
   "metadata": {},
   "source": [
    "## 2. Define Data Paths\n",
    "We set paths to the data directory and metadata files (label and split files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604d698d-ac02-4a01-889a-22fac769a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "label_file = data_dir / \"imagelabels.mat\"\n",
    "split_file = data_dir / \"setid.mat\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814f26e-6762-47fa-9e7f-f56e451730ec",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Metadata\n",
    "We'll load the `.mat` files containig the labels and train/test splits using `scipy.io.loadmat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb9d5e1-da8c-4e10-9fda-db6427dc0353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Feb 19 15:43:33 2009',\n",
       "  '__version__': '1.0',\n",
       "  '__globals__': [],\n",
       "  'labels': array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)},\n",
       " {'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNX86, Created on: Thu Feb 19 17:38:58 2009',\n",
       "  '__version__': '1.0',\n",
       "  '__globals__': [],\n",
       "  'trnid': array([[6765, 6755, 6768, ..., 8026, 8036, 8041]], dtype=uint16),\n",
       "  'valid': array([[6773, 6767, 6739, ..., 8028, 8008, 8030]], dtype=uint16),\n",
       "  'tstid': array([[6734, 6735, 6737, ..., 8044, 8045, 8047]], dtype=uint16)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data = scipy.io.loadmat(label_file)\n",
    "splits_data = scipy.io.loadmat(split_file)\n",
    "\n",
    "labels_data, splits_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a1d6c-1103-4ee3-9d5d-c15cfbbfbcc4",
   "metadata": {},
   "source": [
    "## 4. Extract label and split indices\n",
    "We'll extract the 102 labels of the flower and the indices of the images which define which images belong to which split (train/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "974a8fd8-4424-4441-a7ff-738b2aece535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five labels: [77 77 77 77 77]\n",
      "Min value of train_indices: 28\n",
      "Min value of test_indices: 1\n",
      "Min value of val_indices: 17\n",
      "\n",
      "Length of train_indices: 1020\n",
      "Length of test_indices: 6149\n",
      "Length of val_indices: 1020\n"
     ]
    }
   ],
   "source": [
    "labels = labels_data['labels'].flatten()\n",
    "train_indices = splits_data['trnid'].flatten()\n",
    "test_indices = splits_data['tstid'].flatten()\n",
    "val_indices = splits_data['valid'].flatten()\n",
    "\n",
    "print(f'First five labels: {labels[:5]}')\n",
    "print(f'Min value of train_indices: {train_indices.min()}')\n",
    "print(f'Min value of test_indices: {test_indices.min()}')\n",
    "print(f'Min value of val_indices: {val_indices.min()}')\n",
    "print()\n",
    "print(f'Length of train_indices: {len(train_indices)}')\n",
    "print(f'Length of test_indices: {len(test_indices)}')\n",
    "print(f'Length of val_indices: {len(val_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e88124-ea02-42dc-9fa7-fd27b2a22063",
   "metadata": {},
   "source": [
    "## 5. Map indices to Image Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c4b966f-f1d5-4ebb-967a-059a143b9511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/images/image_00001.jpg'),\n",
       " WindowsPath('data/images/image_00002.jpg'),\n",
       " WindowsPath('data/images/image_00003.jpg'),\n",
       " WindowsPath('data/images/image_00004.jpg'),\n",
       " WindowsPath('data/images/image_00005.jpg')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and sort all the image files\n",
    "data_path = data_dir / \"images\"\n",
    "images_files = sorted(list(data_path.glob(\"*.jpg\")))\n",
    "\n",
    "images_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17e15c6a-1ed2-4cf6-9f02-2fb489ca8187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min train_images value: data\\images\\image_00028.jpg\n",
      "Min test_values: data\\images\\image_00001.jpg\n",
      "Min val_values: data\\images\\image_00017.jpg\n",
      "\n",
      "Length of train_images: 1020\n",
      "Length of test_images 6149\n",
      "Length of val_images: 1020\n"
     ]
    }
   ],
   "source": [
    "# Create lists of image paths and corresponding labels for the training set.\n",
    "train_images = [images_files[i-1] for i in train_indices]  # i-1 adjustment is done for matching the indices from matlab to python's zero based system\n",
    "train_labels = [labels[i-1] for i in train_indices]\n",
    "\n",
    "test_images = [images_files[i-1] for i in test_indices]\n",
    "test_labels = [labels[i-1] for i in test_indices]\n",
    "\n",
    "\n",
    "val_images = [images_files[i-1] for i in val_indices]\n",
    "val_labels = [labels[i-1] for i in val_indices]\n",
    "\n",
    "# Checking the data\n",
    "print(f'Min train_images value: {min(train_images)}')\n",
    "print(f'Min test_values: {min(test_images)}')\n",
    "print(f'Min val_values: {min(val_images)}')\n",
    "print()\n",
    "print(f'Length of train_images: {len(train_images)}')\n",
    "print(f'Length of test_images {len(test_images)}')\n",
    "print(f'Length of val_images: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077dc56-7ea7-45b7-b3bd-af49bf974858",
   "metadata": {},
   "source": [
    "## 6. Define Transformations\n",
    "We use the default transformations for ResNet50, including resizing, cropping, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eec9ffa6-c1dd-44b5-ad85-fab2d4c31ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[232]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "pretrained_weights = models.ResNet50_Weights.DEFAULT\n",
    "common_transforms = pretrained_weights.transforms()\n",
    "\n",
    "\n",
    "common_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0ca32-2dab-441d-95cd-c662c2fc1675",
   "metadata": {},
   "source": [
    "## 7. Create custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d79ff9b-2b62-4074-8235-cd5a689aa2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset: 1020\n",
      "Length of test_dataset 6149\n",
      "Length of val_dataset: 1020\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, labels, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        label = self.labels[idx] - 1\n",
    "        img = transforms.functional.pil_to_tensor(transforms.functional.load_image(data_path))\n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=common_transforms)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=common_transforms)\n",
    "test_dataset = CustomDataset(test_images, test_labels, transform=common_transforms)\n",
    "\n",
    "print(f'Length of train_dataset: {len(train_dataset)}')\n",
    "print(f'Length of test_dataset {len(test_dataset)}')\n",
    "print(f'Length of val_dataset: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72ad6b-a96a-4ba5-9400-ead669e6a413",
   "metadata": {},
   "source": [
    "## 8. Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15821064-3f4f-45e0-bb45-769e2843871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952e216-b890-42dd-9d45-d66c6ae5568a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
